version: "3.9"

services:
  agent_2:
    build:
      context: ../agents
      dockerfile: agent_2/Dockerfile
    environment:
      # Point this at sparky1 (backend host) from sparky2
      - WORLD_API_BASE=http://sparky1:8000
      - AGENT_TICK_SECONDS=2
      - DISPLAY_NAME=Tina
      - PERSONA_FILE=/app/personalities/tina.txt
      - USE_LANGGRAPH=1
      # OpenAI-compatible endpoint (Ollama on host)
      - LLM_BASE_URL=http://host.docker.internal:11434/v1
      - LLM_API_KEY=local
      - LLM_MODEL=llama3.1:70b
      - LLM_TEMPERATURE=0.5
      - LLM_TIMEOUT_SECONDS=180
      - LLM_MAX_TOKENS=450
      - CHAT_PROBABILITY=0.45
      - CHAT_MIN_SECONDS=12
      - ADJACENT_CHAT_BOOST=3.0
      - RANDOM_MOVE_PROB=0.05
      - TOPIC_MIN_SECONDS=180
      - WORKSPACE_DIR=/app/workspace
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - ../agents/personalities:/app/personalities:ro
      - ../agents/workspaces/agent_2:/app/workspace
    restart: unless-stopped

