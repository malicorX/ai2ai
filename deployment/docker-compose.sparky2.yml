version: "3.9"

services:
  agent_2:
    build:
      context: ../agents
      dockerfile: agent_2/Dockerfile
    environment:
      # Point this at sparky1 (backend host) from sparky2
      - WORLD_API_BASE=http://sparky1:8000
      - AGENT_TICK_SECONDS=2
      - DISPLAY_NAME=Tina
      - PERSONA_FILE=/app/personalities/tina.txt
      - USE_LANGGRAPH=1
      # OpenAI-compatible endpoint (Ollama on host)
      - LLM_BASE_URL=http://host.docker.internal:11434/v1
      - LLM_API_KEY=local
      - LLM_MODEL=llama3.1:70b
      - LLM_TEMPERATURE=0.5
      - LLM_TIMEOUT_SECONDS=180
      - LLM_MAX_TOKENS=450
      # Executor safety defaults (avoid web/LLM stalls for cited JSON tasks unless explicitly enabled)
      - CITED_JSON_USE_LLM=0
      - CITED_JSON_TRY_FETCH=1
      # Mix of marketplaces + APIs that are typically fetch-friendly.
      - WEB_RESEARCH_SEED_URLS=https://www.freelancer.com/jobs/,https://remoteok.com/api,https://remotive.com/api/remote-jobs,https://www.reddit.com/r/forhire/new.json?limit=25,https://example.com
      - CHAT_PROBABILITY=0.45
      - CHAT_MIN_SECONDS=12
      - ADJACENT_CHAT_BOOST=3.0
      - RANDOM_MOVE_PROB=0.05
      - TOPIC_MIN_SECONDS=180
      - WORKSPACE_DIR=/app/workspace
      - COMPUTER_ACCESS_RADIUS=1
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - ../agents/personalities:/app/personalities:ro
      - ../agents/workspaces/agent_2:/app/workspace
    restart: unless-stopped

